{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/opt/conda/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Activation, BatchNormalization, Conv2D, Conv3D, Dense, Dropout, GlobalAveragePooling2D, MaxPooling2D, MaxPooling3D\n",
    "from keras.layers import concatenate, Input, merge, Flatten\n",
    "from keras.layers.core import Reshape\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import CSVLogger, EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "from keras.utils import multi_gpu_model \n",
    "from keras.utils import plot_model\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from data_generator import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(K)\n",
    "K.tf.__version__\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_stream(input_shape=(224, 224, 20), verbose=1):\n",
    "    opt_flow_conv = Sequential()\n",
    "  \n",
    "    #conv1\n",
    "    opt_flow_conv.add(Conv2D(filters=96,  \n",
    "                             kernel_size=(7, 7),  \n",
    "                             strides=2, \n",
    "                             padding='same', \n",
    "                             data_format=\"channels_last\",\n",
    "                             input_shape=input_shape)) \n",
    "    \n",
    "    opt_flow_conv.add(BatchNormalization())\n",
    "    opt_flow_conv.add(Activation('relu'))\n",
    "    opt_flow_conv.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    #conv2\n",
    "    opt_flow_conv.add(Conv2D(filters=256, \n",
    "                             kernel_size=(5, 5), \n",
    "                             strides=2, \n",
    "                             padding='same'))\n",
    "    \n",
    "    opt_flow_conv.add(Activation('relu'))\n",
    "    opt_flow_conv.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    #conv3\n",
    "    opt_flow_conv.add(Conv2D(filters=512, \n",
    "                             kernel_size=(3, 3), \n",
    "                             strides=1, \n",
    "                             activation='relu', \n",
    "                             padding='same'))\n",
    "\n",
    "    #conv4\n",
    "    opt_flow_conv.add(Conv2D(filters=512, \n",
    "                             kernel_size=(3, 3), \n",
    "                             strides=1, \n",
    "                             activation='relu', \n",
    "                             padding='same'))\n",
    "\n",
    "    #conv5\n",
    "    opt_flow_conv.add(Conv2D(filters=512, \n",
    "                             kernel_size=(3, 3), \n",
    "                             strides=1, \n",
    "                             activation='relu', \n",
    "                             padding='same'))\n",
    "    \n",
    "    opt_flow_conv.add(Conv2D(filters=512, \n",
    "                             kernel_size=(5, 5), \n",
    "                             strides=1, \n",
    "                             activation='relu', \n",
    "                             padding='same'))\n",
    "    \n",
    "#     opt_flow_conv.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"opt_flow_conv input tensor = {0}\".format(opt_flow_conv.input))\n",
    "        print(\"opt_flow_conv output tensor = {0}\".format(opt_flow_conv.output))\n",
    "    \n",
    "    return opt_flow_conv, opt_flow_conv.output\n",
    "\n",
    "temporal_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_stream(base_model=\"VGG16\", \n",
    "                   input_tensor=Input(shape=(224, 224, 3), name='rgb_img'), \n",
    "                   include_top=False, \n",
    "                   remove_layers=1, \n",
    "                   trainable=1, \n",
    "                   verbose=1):\n",
    "    \n",
    "    if base_model == \"VGG16\":\n",
    "        spatial_conv = VGG16(input_tensor=input_tensor, weights='imagenet', include_top=include_top) \n",
    "    elif base_model == \"ResNet50\":\n",
    "        spatial_conv = ResNet50(input_tensor=input_tensor, weights='imagenet', include_top=include_top) \n",
    "        \n",
    "    for _ in range(remove_layers):\n",
    "        spatial_conv.layers.pop()\n",
    "    \n",
    "    if trainable:\n",
    "        for layer in spatial_conv.layers[0:]:\n",
    "            layer.trainable = True\n",
    "    else:\n",
    "        for layer in spatial_conv.layers[0:]:\n",
    "            layer.trainable = False\n",
    "            \n",
    "    # remove the last \"remove_layers\" layers \n",
    "    output_tensor = spatial_conv.layers[-1].output    \n",
    "    \n",
    "    if verbose:\n",
    "        for i, layer in enumerate(spatial_conv.layers):\n",
    "            print(i, layer.name)\n",
    "            \n",
    "        print(\"spatial_conv input tensor = {0}\".format(spatial_conv.input))\n",
    "        print(\"spatial_conv output tensor = {0}\".format(output_tensor))\n",
    "        \n",
    "\n",
    "    return spatial_conv, output_tensor \n",
    "\n",
    "spatial_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporal model\n",
    "def temporal_cnn(img_shape, opt_flow_len, n_classes):\n",
    "    \n",
    "    img_width = img_shape[0]\n",
    "    img_height = img_shape[1]\n",
    "    \n",
    "    rgb_img = Input(shape=(img_width, img_height, 3), name='rgb_img')\n",
    "\n",
    "    spatial_conv, spatial_output = spatial_stream(input_tensor=rgb_img, remove_layers=1, verbose=0)\n",
    "    print(\"spatial output tensor = \", spatial_output)\n",
    "\n",
    "    temporal_conv, temporal_output= temporal_stream(input_shape=(img_width, img_height, opt_flow_len * 2), verbose=0)\n",
    "    print(\"temporal output tensor = \", temporal_output)\n",
    "\n",
    "    concat_2_stream = concatenate([spatial_output, temporal_output])\n",
    "    print(\"concatenate tensor = \", concat_2_stream.shape)\n",
    "\n",
    "    n_filters = int(concat_2_stream.shape[3] // 2)\n",
    "\n",
    "    concat_2_stream = Conv2D(filters=n_filters, kernel_size=(3, 3), strides=1, padding='same', activation=\"relu\")(concat_2_stream)\n",
    "    print(\"conv2d = \", concat_2_stream)\n",
    "\n",
    "    concat_2_stream = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')(concat_2_stream)\n",
    "    print(\"maxpool2d = \", concat_2_stream)\n",
    "\n",
    "    x = Flatten()(concat_2_stream)\n",
    "    x = Dense(4096)(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    out = Dense(n_classes, activation='softmax')(x)\n",
    "    print(out)\n",
    "\n",
    "    temporal_cnn = Model(inputs=[spatial_conv.input, temporal_conv.input], outputs=out)\n",
    "\n",
    "    # merged = Merge([left_branch, middle_branch, right_branch], mode='concat')\n",
    "\n",
    "    # final_model = Sequential()\n",
    "    # final_model.add(merged)\n",
    "    # final_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # plot_model(model, to_file='demo.png', show_shapes=True)\n",
    "    print(temporal_cnn.summary())\n",
    "    return temporal_cnn\n",
    "\n",
    "# n_frames_per_video = 5\n",
    "# n_classes = 4\n",
    "# img_shape = (64, 64)\n",
    "# opt_flow_len = 10\n",
    "# temporal_cnn(img_shape, opt_flow_len, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_cnn_train(num_of_snip=1, \n",
    "                       opt_flow_len=10, \n",
    "                       image_shape=(224, 224),\n",
    "                       batch_size=32, \n",
    "                       nb_epoch=100, \n",
    "                       saved_model=None,\n",
    "                       class_limit=None, \n",
    "                       load_to_memory=False, \n",
    "                       name_str=None, \n",
    "                       gpus=1):\n",
    "    \n",
    "    # Get local time.\n",
    "    time_str = time.strftime(\"%Y%m%d%H%M\", time.localtime())\n",
    "\n",
    "    if name_str == None:\n",
    "        name_str = \"temporal_cnn\"\n",
    "#         name_str = time_str\n",
    "\n",
    "    # Callbacks: Save the model.\n",
    "    saved_model_dir = os.path.join('out', 'checkpoints', name_str)\n",
    "    \n",
    "    if not os.path.exists(saved_model_dir):\n",
    "        os.makedirs(saved_model_dir)\n",
    "            \n",
    "#     checkpointer = ModelCheckpoint(filepath=os.path.join(saved_model_dir, '{epoch:03d}-{val_loss:.3f}.hdf5'),\n",
    "    checkpointer = ModelCheckpoint(filepath=os.path.join(saved_model_dir, 'temproal_cnn_model.hdf5'),\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True, \n",
    "                                   monitor=\"loss\")\n",
    "\n",
    "    # Callbacks: TensorBoard\n",
    "    TensorBoard_dir = os.path.join('out', 'TB', name_str)\n",
    "    \n",
    "    if not os.path.exists(TensorBoard_dir):\n",
    "            os.makedirs(TensorBoard_dir)\n",
    "            \n",
    "    tb = TensorBoard(log_dir=os.path.join(TensorBoard_dir))\n",
    "\n",
    "    # Callbacks: Early stopper.\n",
    "    early_stopper = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "    # Callbacks: Save results.\n",
    "    log_dir = os.path.join('out', 'logs', name_str)\n",
    "    \n",
    "    if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "            \n",
    "    timestamp = time.time()\n",
    "    csv_logger = CSVLogger(os.path.join(log_dir, 'temporal_cnn_training-' + str(timestamp) + '.log'))\n",
    "\n",
    "    # Callbacks: learning rate\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, min_lr=0.00001)\n",
    "    \n",
    "    # Learning rate schedule.\n",
    "#     lr_schedule = LearningRateScheduler(fixed_schedule, verbose=0)\n",
    "\n",
    "    # Get the data and process it.\n",
    "    if image_shape is None:\n",
    "        data = DataSet(num_of_snip=num_of_snip,\n",
    "                       opt_flow_len=opt_flow_len,\n",
    "                       class_limit=class_limit)\n",
    "    else:\n",
    "        data = DataSet(num_of_snip=num_of_snip,\n",
    "                       opt_flow_len=opt_flow_len,\n",
    "                       image_shape=image_shape,\n",
    "                       class_limit=class_limit)\n",
    "        \n",
    "#     print(\"temporal-train-, Show data list: {0}\".format(data.data_list))\n",
    "    \n",
    "    # Get samples per epoch.\n",
    "    # Multiply by 0.7 to attempt to guess how much of data.data is the train set.\n",
    "#     steps_per_epoch = (len(data.data_list) * 0.7) // batch_size\n",
    "\n",
    "    steps_per_epoch = batch_size\n",
    "    \n",
    "    if load_to_memory:\n",
    "        # Get data.\n",
    "        X, y = data.get_all_stacks_in_memory('train')\n",
    "        X_test, y_test = data.get_all_stacks_in_memory('test')\n",
    "    else:\n",
    "        # Get generators.\n",
    "        generator = data.stack_generator(batch_size, 'train')\n",
    "        val_generator = data.stack_generator(batch_size, 'test', name_str=name_str)\n",
    "        \n",
    "    # Get the model.    \n",
    "    # Replicates `model` on 4 GPUs. \n",
    "    # This assumes that your machine has 4 available GPUs. \n",
    "    nb_classes = len(data.classes)\n",
    "\n",
    "    metrics = ['accuracy']\n",
    "    if nb_classes >= 10:\n",
    "            metrics.append('top_k_categorical_accuracy')\n",
    "\n",
    "    optimizer = SGD(lr=1e-2, momentum=0.9, nesterov=True)\n",
    "\n",
    "    \n",
    "    if saved_model is not None:\n",
    "        tmp_cnn = load_model(saved_model)\n",
    "    else:\n",
    "        \n",
    "        if gpus > 1:\n",
    "            print(\"# of GPUS:\", gpus)\n",
    "\n",
    "            tmp_model = temporal_cnn(img_shape=image_shape, opt_flow_len=opt_flow_len, n_classes=nb_classes)\n",
    "            tmp_cnn = multi_gpu_model(tmp_model, gpus=gpus)\n",
    "        else:\n",
    "            print(\"# of GPUS:\", gpus)\n",
    "\n",
    "            tmp_model = temporal_cnn(img_shape=image_shape, opt_flow_len=opt_flow_len, n_classes=nb_classes)\n",
    "            tmp_cnn = tmp_model\n",
    "\n",
    "        tmp_cnn.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fit!\n",
    "    if load_to_memory:\n",
    "        # Use standard fit.\n",
    "#         temporal_cnn.model.fit(\n",
    "        tmp_cnn.fit(X,\n",
    "                    y,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=[tb, early_stopper, csv_logger],\n",
    "                    epochs=nb_epoch)\n",
    "    else:\n",
    "        # Use fit generator.\n",
    "        print(\"temporal-train-, use fit generator\")\n",
    "#         temporal_cnn.model.fit_generator(\n",
    "        tmp_cnn.fit_generator(\n",
    "                generator=generator,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                epochs=nb_epoch,\n",
    "                verbose=1,\n",
    "                callbacks=[tb, early_stopper, csv_logger, checkpointer, reduce_lr],\n",
    "                validation_data=val_generator,\n",
    "                validation_steps=1,\n",
    "                max_queue_size=20,\n",
    "                workers=1,\n",
    "                use_multiprocessing=False)\n",
    "        \n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def main(gpus=2):\n",
    "    \"\"\"\n",
    "    These are the main training settings. \n",
    "    Set each before running this file.\n",
    "    \"\"\"\n",
    "    \"==============================================================================\"\n",
    "    config = {\"saved_model\": os.path.join('out', 'checkpoints', 'temporal_cnn', 'temproal_cnn_model.hdf5')}\n",
    "\n",
    "    saved_model = None\n",
    "    class_limit = None  # int, can be 1-101 or None\n",
    "    # yuzhe 20180110\n",
    "    num_of_snip = 1 # number of chunks(snippets) used for each video\n",
    "    opt_flow_len = 10 # number of optical flow frames used\n",
    "#     image_shape=(224, 224)\n",
    "#     image_shape=(64, 64)\n",
    "\n",
    "    image_shape=(32, 32)\n",
    "\n",
    "    \n",
    "    load_to_memory = False  # pre-load the sequences into memory\n",
    "    batch_size = 128\n",
    "\n",
    "    nb_epoch = 2222\n",
    "#    nb_epoch = 3333\n",
    "    name_str = None\n",
    "\n",
    "    if os.path.exists(config[\"saved_model\"]):\n",
    "#         saved_model = load_model(config[\"saved_model\"])\n",
    "        saved_model = config[\"saved_model\"]\n",
    "\n",
    "        print(\"using saved model\")\n",
    "    \n",
    "    temporal_cnn_train(num_of_snip=num_of_snip, \n",
    "                       opt_flow_len=opt_flow_len, \n",
    "                       saved_model=saved_model,\n",
    "                       class_limit=class_limit, \n",
    "                       image_shape=image_shape,\n",
    "                       load_to_memory=load_to_memory, \n",
    "                       batch_size=batch_size,\n",
    "                       nb_epoch=nb_epoch, \n",
    "                       name_str=name_str,\n",
    "                       gpus=gpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['accuracy']\n",
    "\n",
    "# optimizer = SGD(lr=1e-2, momentum=0.9, nesterov=True)\n",
    "\n",
    "optimizer = Adam()\n",
    "temporal_cnn.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=metrics)\n",
    "# spatiotemporal_cnn.fit(x=[input1, input2], y=a, batch_size=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 32\n",
    "generator = gen_dataset.stack_generator(batch_size, 'train')\n",
    "\n",
    "val_generator = gen_dataset.stack_generator(batch_size, 'test')\n",
    "\n",
    "spatiotemporal_cnn.fit_generator(\n",
    "                generator=generator,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                epochs=nb_epoch,\n",
    "                verbose=1,\n",
    "#                 callbacks=[tb, early_stopper, csv_logger, checkpointer, lr_schedule, reduce_lr],\n",
    "                validation_data=val_generator,\n",
    "                validation_steps=1,\n",
    "                max_queue_size=20,\n",
    "                workers=1,\n",
    "                use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (224, 224, 3)\n",
    "\n",
    "num_of_snip = 5\n",
    "# rgb_img = Input(shape=(224, 224, 3), name='rgb_img')\n",
    "rgb_img = Input(shape=(224, 224, 3), name='rgb_img')\n",
    "opt_flow = Input(shape=(224, 224, 20), name='opt_flow')\n",
    "\n",
    "#===============================================================================\n",
    "spatial_inputs = []\n",
    "temporal_inputs = []\n",
    "# multiple_inputs = []\n",
    "output_tensors = []\n",
    "for snip_idx in range(num_of_snip):\n",
    "    spatial_conv, spatial_output = spatial_stream(input_tensor=Input(shape=(224, 224, 3), name='rgb_img_{0}'.format(snip_idx)), \n",
    "                                                  remove_layers=1, \n",
    "                                                  verbose=0)\n",
    "    \n",
    "    print(\"spatial output tensor = \", spatial_output)\n",
    "\n",
    "    temporal_conv, temporal_output= temporal_stream(input_shape=(224, 224, 20), verbose=0)\n",
    "    print(\"temporal output tensor = \", temporal_output)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     multiple_inputs.append(spatial_conv.input)\n",
    "#     multiple_inputs.append(temporal_conv.input)\n",
    "    spatial_inputs.append(spatial_conv.input)\n",
    "    temporal_inputs.append(temporal_conv.input)\n",
    "\n",
    "    output_tensors.append(concatenate([spatial_output, temporal_output]))\n",
    "\n",
    "\n",
    "spatial_inputs.append(spatial_conv_snip_5.input)\n",
    "temporal_inputs.append(temporal_conv_snip_5.input)\n",
    "output_tensors.append(concatenate([spatial_output_snip_5, temporal_output_snip_5]))\n",
    "\n",
    "multiple_inputs = spatial_inputs + temporal_inputs\n",
    "# print(multiple_inputs)\n",
    "# print(output_tensors)\n",
    "\n",
    "\n",
    "# concat_2_stream = K.stack(output_tensors, axis=3) # [Width, Height, Time, Channel]\n",
    "concat_2_stream = K.stack(output_tensors, axis=1) # [?, Time, Width, Height, Channel]\n",
    "\n",
    "# concat_2_stream = concatenate([spatial_output, temporal_output])\n",
    "print(\"concatenate tensor = \", concat_2_stream.shape)\n",
    "\n",
    "# concat_2_stream = Activation(\"relu\")(concat_2_stream)\n",
    "# print(\"concatenate tensor after activation = \", concat_2_stream.shape)\n",
    "\n",
    "n_filters = int(concat_2_stream.shape[4] // 2)\n",
    "print(type(n_filters))\n",
    "# concat_2_stream = Conv2D(filters=n_filters, kernel_size=(1, 1), strides=1, padding='same')(concat_2_stream)\n",
    "# print(\"concatenate tensor after 1 x 1 convolution = \", concat_2_stream.shape)\n",
    "\n",
    "Conv3D_2_stream = Conv3D(filters=n_filters, # 3d convolution over Time, Width, Height\n",
    "                         kernel_size=(3, 3, 3), # specifying the depth (<< notice this), height and width of the 3D convolution window. \n",
    "                         strides=(1, 1, 1), \n",
    "#                          padding='valid', \n",
    "                         padding='same', \n",
    "                         data_format=\"channels_last\", \n",
    "#                          dilation_rate=(1, 1, 1), \n",
    "                         activation=\"relu\", \n",
    "                         use_bias=True)(concat_2_stream)\n",
    "\n",
    "print(\"Conv3D output tensor\" , Conv3D_2_stream)\n",
    "\n",
    "\n",
    "MaxPool3D_2_stream = MaxPooling3D(pool_size=(num_of_snip, 2, 2), strides=(2, 2, 2), padding=\"valid\")(Conv3D_2_stream)\n",
    "#         pool3d = tf.layers.max_pooling3d(fusion_conv6, pool_size=[self.nFramesPerVid,2,2], strides=(2,2,2), padding='valid') # [?,1,7,7,512]  ?=batchsize \n",
    "print(\"MaxPool3D output tensor\", MaxPool3D_2_stream)\n",
    "\n",
    "# # MaxPool3D_2_stream = Reshape(target_shape=(13 * 13 * 512))(MaxPool3D_2_stream)\n",
    "# # print(\"MaxPool3D output tensor after reshape\", MaxPool3D_2_stream)\n",
    "\n",
    "\n",
    "# x = Flatten(input_shape=MaxPool3D_2_stream.shape[1:])(MaxPool3D_2_stream)\n",
    "x = Flatten()(MaxPool3D_2_stream)\n",
    "\n",
    "print(\"Flattened tensor\", x.shape)\n",
    "# x = Flatten(concat_2_stream)\n",
    "\n",
    "x = Dense(4096)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "x = Activation('elu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(2048, activation='relu')(x)\n",
    "print(x)\n",
    "out = Dense(10, activation='softmax')(x)\n",
    "print(out)\n",
    "\n",
    "# model = Model(inputs=[rgb_img, opt_flow], outputs=out)\n",
    "# spatiotemporal_cnn = Model(inputs=[spatial_conv.input, temporal_conv.input], outputs=out)\n",
    "spatiotemporal_cnn = Model(inputs=multiple_inputs, outputs=out)\n",
    "\n",
    "# spatiotemporal_cnn = Model(inputs=[rgb_img, opt_flow], outputs=out)\n",
    "\n",
    "\n",
    "# merged = Merge([left_branch, middle_branch, right_branch], mode='concat')\n",
    "\n",
    "# final_model = Sequential()\n",
    "# final_model.add(merged)\n",
    "# final_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "# plot_model(model, to_file='demo.png', show_shapes=True)\n",
    "print(spatiotemporal_cnn.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_snip=1\n",
    "opt_flow_len=10 \n",
    "saved_model=None\n",
    "class_limit=None \n",
    "image_shape=(64, 64)\n",
    "load_to_memory=False\n",
    "batch_size=32\n",
    "nb_epoch=100\n",
    "name_str=None\n",
    "\n",
    "gen_dataset = DataSet(num_of_snip=num_of_snip,\n",
    "                      opt_flow_len=opt_flow_len,\n",
    "                      image_shape=image_shape,\n",
    "                      class_limit=class_limit)\n",
    "\n",
    "\n",
    "generator = gen_dataset.stack_generator(batch_size, 'train')\n",
    "val_generator = gen_dataset.stack_generator(batch_size, 'test')\n",
    "\n",
    "# print(next(generator))\n",
    "# tmp_list, tmp_cls = next(generator)\n",
    "\n",
    "# print(tmp_list[0].shape)\n",
    "# print(tmp_list[1].shape)\n",
    "# print(tmp_cls.shape)\n",
    "# print(tmp_cls)\n",
    "\n",
    "# for _ in range(10000):\n",
    "#     next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_list, tmp_cls = next(generator)\n",
    "print(len(tmp_list))\n",
    "print(tmp_cls.shape)\n",
    "# print(len(tmp_list[0]))\n",
    "# print(tmp_list[0][batch_size - 1][0].shape)\n",
    "# print(len(tmp_list[1]))\n",
    "# print(tmp_list[1][batch_size - 1][0].shape)\n",
    "# print(tmp_cls.shape)\n",
    "# print(tmp_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "log_dir = os.path.join(os.getcwd(), 'out', 'logs', 'temporal_cnn')\n",
    "for tmp in os.listdir(log_dir):\n",
    "    if tmp[0] != \".\":\n",
    "        \n",
    "        log_data = pd.read_csv(log_dir + \"/\" + tmp)\n",
    "        \n",
    "        print(log_data.columns)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        ax1 = plt.subplot2grid(shape=(2, 4), loc=(0, 0), rowspan=2, colspan=2)\n",
    "        ax1.set_xlabel(\"epoch\")\n",
    "        ax1.set_ylabel(\"loss\")\n",
    "        lg1 = ax1.plot(log_data[\"epoch\"], log_data[\"loss\"])\n",
    "        lg2 = ax1.plot(log_data[\"epoch\"], log_data[\"val_loss\"], color=\"blue\")\n",
    "        ax1.legend()\n",
    "\n",
    "        ax2 = plt.subplot2grid(shape=(2, 4), loc=(0, 2), rowspan=2, colspan=2)\n",
    "        ax2.set_xlabel(\"epoch\")\n",
    "        ax2.set_ylabel(\"accuracy\")\n",
    "        lg2 = ax2.plot(log_data[\"epoch\"], log_data[\"acc\"])\n",
    "        lg2 = ax2.plot(log_data[\"epoch\"], log_data[\"val_acc\"], color=\"blue\")\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "# line, = ax.plot([1, 2, 3])\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "\n",
    "saved_model = os.path.join(os.getcwd(), 'out', 'checkpoints', 'temporal_cnn', 'temproal_cnn_model_no_dropout_1.hdf5')\n",
    "print(saved_model)\n",
    "\n",
    "batch_size = 1000\n",
    "class_limit = None  # int, can be 1-101 or None\n",
    "# yuzhe 20180110\n",
    "num_of_snip = 1 # number of chunks(snippets) used for each video\n",
    "opt_flow_len = 10 # number of optical flow frames used\n",
    "#     image_shape=(224, 224)\n",
    "image_shape=(64, 64)\n",
    "\n",
    "# model = temporal_cnn(img_shape, opt_flow_len, n_classes=5)\n",
    "\n",
    "\n",
    "config={\"etc\": \"./etc\", \n",
    "        \"data_list\": \"data_list_v1.csv\", \n",
    "     \"img_path\": \"/home/jovyan/at073-group20/20bn_jester_500/train\",                          \n",
    "#                          \"img_path\": \"/home/jovyan/at073-group20/gesture2img\", \n",
    "     \"opt_flow_path\": \"/home/jovyan/at073-group20/20bn_jester_500/optflow/train\"}\n",
    "#                          \"opt_flow_path\": \"/home/jovyan/at073-group20/gesture2flow_s224\"})\n",
    "\n",
    "gen_dataset = DataSet(num_of_snip=num_of_snip,\n",
    "                      opt_flow_len=opt_flow_len,\n",
    "                      image_shape=image_shape, config=config)\n",
    "\n",
    "val_generator = gen_dataset.stack_generator(batch_size, 'test')\n",
    "\n",
    "model = load_model(saved_model)\n",
    "\n",
    "val_classes = gen_dataset.classes\n",
    "print(val_classes)\n",
    "\n",
    "one_hot_idx = [gen_dataset.get_class_one_hot(tmp) for tmp in val_classes]\n",
    "print(one_hot_idx)\n",
    "\n",
    "\n",
    "\n",
    "# pd.replace\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for predict_idx in range(1):\n",
    "    X, y = next(val_generator)\n",
    "    print(len(X))\n",
    "    \n",
    "    preds = model.predict(X)\n",
    "    \n",
    "    for tmp_idx in range(batch_size):\n",
    "        y_true.append(val_classes[np.argmax(y[tmp_idx])])\n",
    "        y_pred.append(val_classes[np.argmax(preds[tmp_idx])])\n",
    "#         print(val_classes[np.argmax(y[tmp_idx])], val_classes[np.argmax(preds[tmp_idx])])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "%matplotlib inline\n",
    "# Put the results in\n",
    "# y_true = \n",
    "# y_pred = \n",
    "\n",
    "\n",
    "labels = sorted(list(set(y_true)))\n",
    "cmx_d = confusion_matrix(y_pred, y_true, labels=labels)\n",
    "cmxn_d = cmx_d.astype('float') / cmx_d.sum(axis=0)[np.newaxis ,:]\n",
    "cmx_df = pd.DataFrame(cmx_d, index=labels, columns=labels)\n",
    "cmxn_df = pd.DataFrame(cmxn_d, index=labels, columns=labels)\n",
    "\n",
    "plt.figure(figsize = (14.5, 7))\n",
    "plt.subplot(121)\n",
    "sns.heatmap(cmxn_df, annot=True, cmap='YlGnBu', cbar=False)\n",
    "plt.title('Normalized confusion matrix')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.subplot(122)\n",
    "sns.heatmap(cmx_df, annot=True, cmap='YlGnBu', cbar=False)\n",
    "plt.title('Confusion matrix')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
